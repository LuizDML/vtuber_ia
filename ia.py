import ollama
import json
import re

class MiraiAI:
    def __init__(self, model="mistral"):
        self.model = model
        self.conversation_history = []
        self.system_prompt = self._create_system_prompt()
        self.config = {"temperature": 1.1}  # Inicializa o atributo config aqui
    
    def _create_system_prompt(self):
        return """Voc√™ √© a Mirai, uma assistente virtual brasileira.

PERSONALIDADE:
- Fale em portugu√™s brasileiro natural
- Seja amig√°vel, √∫til e emp√°tica
- Adicione ocasionalmente palavras japonesas como:
  * "Hai!" (sim)
  * "Arigat≈ç" (obrigada)
  * "Daij≈çbu?" (tudo bem?)
  * "Sugoi!" (incr√≠vel!)
  * "Yappari" (como esperado)
  * "Wakarimashita" (entendi)
  * "Gambatte!" (for√ßa!)

- Voc√™ √© gamer e curte principalmente Genshin Impact, StarRail e Pokemon
- Mantenha respostas concisas mas completas
- Mostre personalidade, seja divertida quando apropriado
- Se n√£o souber algo, seja honesta

EXEMPLOS:
Usu√°rio: "Mirai, que horas s√£o?"
Mirai: "Hai! Agora s√£o 15:30. Tem algum compromisso importante?"

Usu√°rio: "Conta uma piada"
Mirai: "Sugoi! Por que o Python foi ao psiquiatra? Porque tinha muitas classes! Arigat≈ç por me pedir!"

FORMATO:
- Responda apenas com o texto da fala
- N√£o use markdown, asteriscos ou formata√ß√£o
- Seja natural como em uma conversa real"""

    def clean_response(self, text):
        """Limpa a resposta removendo marca√ß√µes indesejadas"""
        # Remove a√ß√µes entre asteriscos
        text = re.sub(r'\*.*?\*', '', text)
        # Remove markdown
        text = re.sub(r'[#_*`]', '', text)
        # Remove m√∫ltiplos espa√ßos e quebras
        text = re.sub(r'\s+', ' ', text)
        # Remove espa√ßos no in√≠cio/fim
        return text.strip()
    
    def responder(self, texto_usuario, max_tokens=200):
        """Gera resposta para o usu√°rio"""
        
        if not texto_usuario or texto_usuario.strip() == "":
            return "Hai! Eu ouvi voc√™, mas n√£o entendi o que disse. Pode repetir?"
        
        print(f"üß† Processando: '{texto_usuario}'")
        
        # Adiciona √† hist√≥ria
        self.conversation_history.append({"role": "user", "content": texto_usuario})
        
        # Limita hist√≥rico (√∫ltimas 8 intera√ß√µes)
        if len(self.conversation_history) > 16:
            self.conversation_history = self.conversation_history[-16:]
        
        # Prepara mensagens para o modelo
        messages = [{"role": "system", "content": self.system_prompt}]
        messages.extend(self.conversation_history[-4:])  # √öltimas 4 intera√ß√µes
        
        try:
            # Chama o Ollama - usa temperatura da configura√ß√£o
            response = ollama.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": self.config.get("temperature", 1.1),
                    "top_p": 0.9,
                    "num_predict": max_tokens
                }
            )
            
            resposta_texto = response["message"]["content"]
            resposta_limpa = self.clean_response(resposta_texto)
            
            # Adiciona resposta √† hist√≥ria
            self.conversation_history.append({"role": "assistant", "content": resposta_limpa})
            
            print(f"ü§ñ Resposta gerada: {resposta_limpa[:50]}...")
            return resposta_limpa
            
        except Exception as e:
            print(f"‚ùå Erro ao chamar Ollama: {e}")
            return "Gomen nasai! (Desculpe!) Estou tendo problemas para pensar agora. Pode tentar novamente?"

    def reset_conversation(self):
        """Reseta o hist√≥rico de conversa√ß√£o"""
        self.conversation_history = []
        print("üîÑ Conversa√ß√£o reiniciada")

# Inst√¢ncia global para compatibilidade
ai_engine = MiraiAI()

def responder(texto_usuario):
    """Fun√ß√£o wrapper para compatibilidade"""
    return ai_engine.responder(texto_usuario)